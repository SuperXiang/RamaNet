{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backbone.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PWjHx8nrQlPO",
        "cWiksT9EQqat",
        "Vaect4G7Qq5b"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIdr3XpL7Qoh"
      },
      "source": [
        "# **Setup Environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS901OrhOWKo"
      },
      "source": [
        "!wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install --channel defaults conda python=3.6 --yes\n",
        "!conda update --channel defaults --all --yes\n",
        "import sys\n",
        "_ = (sys.path.append(\"/usr/local/lib/python3.6/site-packages\"))\n",
        "!pip3 install biopython parameter-sherpa\n",
        "!conda install --channel https://levinthal:paradox@conda.graylab.jhu.edu pyrosetta --yes\n",
        "!apt install dssp\n",
        "!rm -r sample_data/\n",
        "!rm Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!wget https://www.dropbox.com/s/1ne938re177ld6o/PS%2BCM.hdf5.xz?dl=0\n",
        "!mv PS+CM.hdf5.xz?dl=0 PS+CM.hdf5.xz\n",
        "!xz -d PS+CM.hdf5.xz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJjv2NgkOgIO"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import h5py\n",
        "import time\n",
        "import glob\n",
        "import math\n",
        "import keras\n",
        "import sklearn\n",
        "import Bio.PDB\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from pyrosetta import *\n",
        "from pyrosetta.toolbox import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential, Model\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers import Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import UpSampling2D, BatchNormalization\n",
        "from keras.layers import Dropout, GlobalMaxPooling2D, Conv2DTranspose\n",
        "init('-out:level 0')\n",
        "\n",
        "def SQM(filename):\n",
        "\tparser = Bio.PDB.PDBParser()\n",
        "\tstructure = parser.get_structure('{}'.format(filename), filename)\n",
        "\tdssp = Bio.PDB.DSSP(structure[0], filename, acc_array='Wilke')\n",
        "\tAminoAcid = {\t'A':129, 'P':159, 'N':195, 'H':224,\n",
        "\t\t\t\t\t'V':174, 'Y':263, 'C':167, 'K':236,\n",
        "\t\t\t\t\t'I':197, 'F':240, 'Q':225, 'S':155,\n",
        "\t\t\t\t\t'L':201, 'W':285, 'E':223, 'T':172,\n",
        "\t\t\t\t\t'M':224, 'R':274, 'G':104, 'D':193}\n",
        "\tsec_struct = []\n",
        "\tSASA = []\n",
        "\tfor aa in dssp:\n",
        "\t\tif   aa[2] == 'G' or aa[2] == 'H' or aa[2] == 'I': ss = 'H'\n",
        "\t\telif aa[2] == 'B' or aa[2] == 'E':                 ss = 'S'\n",
        "\t\telif aa[2] == 'S' or aa[2] == 'T' or aa[2] == '-': ss = 'L'\n",
        "\t\tsec_struct.append(ss)\n",
        "\t\tsasa = AminoAcid[aa[1]]*aa[3]\n",
        "\t\tif sasa <= 25:      sasa = 'C'\n",
        "\t\telif 25 < sasa < 40:sasa = 'B'\n",
        "\t\telif sasa >= 40:    sasa = 'S'\n",
        "\t\tSASA.append(sasa)\n",
        "\t''' Secondary structure measurement '''\n",
        "\tH = len([x for x in sec_struct if x == 'H'])\n",
        "\tS = len([x for x in sec_struct if x == 'S'])\n",
        "\tL = len([x for x in sec_struct if x == 'L'])\n",
        "\ttotal = len(sec_struct)\n",
        "\tratio = (H+S)/total\n",
        "\tlimit = 1\n",
        "\tslope = 10\n",
        "\tbias  = 0.5\n",
        "\tSS = limit/(1+np.exp(slope*(bias-ratio)))\n",
        "\t''' SASA measurement '''\n",
        "\tsurface = len([x for x in SASA if x == 'S'])\n",
        "\tboundery = len([x for x in SASA if x == 'B'])\n",
        "\tin_core = len([x for x in SASA if x == 'C'])\n",
        "\ttotal = len(SASA)\n",
        "\tpercent = (in_core*100)/total\n",
        "\tCore = (2.50662/math.sqrt(2*(math.pi)))*math.exp(-((percent-30)**2)/100)\n",
        "\t''' Radius of gyration measurement '''\n",
        "\tcoord = list()\n",
        "\tmass = list()\n",
        "\tStructure = open(filename, 'r')\n",
        "\tfor line in Structure:\n",
        "\t\ttry:\n",
        "\t\t    line = line.split()\n",
        "\t\t    x = float(line[6])\n",
        "\t\t    y = float(line[7])\n",
        "\t\t    z = float(line[8])\n",
        "\t\t    coord.append([x, y, z])\n",
        "\t\t    if   line[-1] == 'C': mass.append(12.0107)\n",
        "\t\t    elif line[-1] == 'O': mass.append(15.9994)\n",
        "\t\t    elif line[-1] == 'N': mass.append(14.0067)\n",
        "\t\t    elif line[-1] == 'S': mass.append(32.065)\n",
        "\t\texcept: pass\n",
        "\txm = [(m*i, m*j, m*k) for (i, j, k), m in zip(coord, mass)]\n",
        "\ttmass = sum(mass)\n",
        "\trr = sum(mi*i + mj*j + mk*k for (i, j, k), (mi, mj, mk) in zip(coord, xm))\n",
        "\tmm = sum((sum(i)/tmass)**2 for i in zip(*xm))\n",
        "\trg = math.sqrt(rr/tmass-mm)\n",
        "\tRg = (2.50662/math.sqrt(2*(math.pi)))*math.exp(-((rg-12)**2)/40)\n",
        "\t''' The metric '''\n",
        "\tTheMetric = sum([SS, Core, Rg])/3\n",
        "\tif TheMetric <= 0.8: choice = False\n",
        "\telse: choice = True\n",
        "\treturn(round(TheMetric, 5), choice)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWjHx8nrQlPO"
      },
      "source": [
        "# **PS+CM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZM-0heQOQE4"
      },
      "source": [
        "def fold(P, S, C):\n",
        "    ''' Folds a structure using phi/psi angles and contact map '''\n",
        "    P = np.ndarray.tolist(P)\n",
        "    S = np.ndarray.tolist(S)\n",
        "    size = int(len(P))\n",
        "    Vs = []\n",
        "    for numb in range(size): Vs.append('A')\n",
        "    sequence = ''.join(Vs)\n",
        "    pose = pose_from_sequence(sequence)\n",
        "    for count, (phi, psi) in enumerate(zip(P, S)):\n",
        "        pose.set_phi(count+1, float(phi))\n",
        "        pose.set_psi(count+1, float(psi))\n",
        "    pose.dump_pdb('angles.pdb')\n",
        "    structure = Bio.PDB.PDBParser().get_structure('angles', 'angles.pdb')\n",
        "    dssp = Bio.PDB.DSSP(structure[0], 'angles.pdb', acc_array='Wilke')\n",
        "    ppb = Bio.PDB.Polypeptide.PPBuilder()\n",
        "    chain = ppb.build_peptides(structure, aa_only=False)[0]\n",
        "    SS = []\n",
        "    for aa in dssp:\n",
        "        if aa[2] == 'G' or aa[2] == 'H' or aa[2] == 'I': SSname = 'H'\n",
        "        elif aa[2] == 'B' or aa[2] == 'E': SSname = 'S'\n",
        "        else: SSname = 'L'\n",
        "        SS.append(SSname)\n",
        "    try:\n",
        "        for i in enumerate(reversed(SS)):\n",
        "            if i[1] != 'L':\n",
        "                num = i[0]\n",
        "                break\n",
        "        for model in structure:\n",
        "            for chain in model:\n",
        "                for i in reversed(range(150-num+1, 150+1)):\n",
        "                    chain.detach_child((' ', i, ' '))\n",
        "        io = Bio.PDB.PDBIO()\n",
        "        io.set_structure(structure)\n",
        "        io.save('turnicated.pdb')\n",
        "        os.remove('angles.pdb')\n",
        "        pose = pose_from_pdb('turnicated.pdb')\n",
        "    except:\n",
        "        os.remove('angles.pdb')\n",
        "        print('[-] Generated structure not satisfactory')\n",
        "        exit()\n",
        "    size = pose.residues.__len__()\n",
        "    with open('constraints.cst', 'w') as thefile:\n",
        "        for a in range(1, size+1):\n",
        "            for A in range(1, size+1):\n",
        "                if C[a][A] !=0:\n",
        "                    line = 'AtomPair CA {} CA {} GAUSSIANFUNC {} 1.0\\n'\\\n",
        "                    .format(a, A, C[a][A])\n",
        "                    thefile.write(line)\n",
        "    con = pyrosetta.rosetta.protocols.constraint_movers.ConstraintSetMover()\n",
        "    con.constraint_file('constraints.cst')\n",
        "    con.add_constraints(True)\n",
        "    con.apply(pose)\n",
        "    scorefxn = get_fa_scorefxn()\n",
        "    score_manager = pyrosetta.rosetta.core.scoring.ScoreTypeManager()\n",
        "    atom_pair_constraint = score_manager.score_type_from_name('atom_pair_constraint')\n",
        "    rama_prepro = score_manager.score_type_from_name('rama_prepro')\n",
        "    scorefxn.set_weight(atom_pair_constraint, 5)\n",
        "    scorefxn.set_weight(rama_prepro, 5)\n",
        "    relax = pyrosetta.rosetta.protocols.relax.FastRelax()\n",
        "    relax.set_scorefxn(scorefxn)\n",
        "    os.remove('turnicated.pdb')\n",
        "    os.remove('constraints.cst')\n",
        "    relax.apply(pose)\n",
        "    pose.dump_pdb('backbone.pdb')\n",
        "\n",
        "lrG = 0.0004737496898081273\n",
        "lrD = 9.87524413309367e-05\n",
        "nodeG = 4\n",
        "nodeD = 4\n",
        "moment = 0.99\n",
        "alpha = 0.2596531963200861\n",
        "drop = 0.47267237561710007\n",
        "kernel = 3\n",
        "stride = 2\n",
        "latent = 128\n",
        "batchs = 256\n",
        "epochs = 10000\n",
        "C_MAX = 12\n",
        "with h5py.File('PS+CM.hdf5', 'r') as data: dataset=data['default'][()]\n",
        "dataset = np.reshape(dataset, (-1, dataset.shape[1], dataset.shape[2], 1))\n",
        "shape = dataset.shape[1:]\n",
        "print(dataset.shape)\n",
        "G = Sequential()\n",
        "G.add(Dense(2**(nodeG+1) * 75 * 38, activation='relu',input_dim=latent))\n",
        "G.add(Reshape((75, 38, 2**(nodeG+1))))\n",
        "G.add(UpSampling2D(size=(1, 2)))\n",
        "G.add(Conv2D(2**(nodeG+1), kernel_size=kernel, padding='same'))\n",
        "G.add(BatchNormalization(momentum=moment))\n",
        "G.add(Activation('relu'))\n",
        "G.add(UpSampling2D())\n",
        "G.add(Conv2D(2**(nodeG+0), kernel_size=kernel, padding='same'))\n",
        "G.add(BatchNormalization(momentum=moment))\n",
        "G.add(Activation('relu'))\n",
        "G.add(Conv2D(1, kernel_size=kernel, padding='same'))\n",
        "G.add(Activation('tanh'))\n",
        "D = Sequential()\n",
        "D.add(Conv2D(2**(nodeD+0), kernel_size=kernel, strides=stride, input_shape=shape, padding='same'))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Conv2D(2**(nodeD+1), kernel_size=kernel, strides=stride, padding='same'))\n",
        "D.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "D.add(BatchNormalization(momentum=moment))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Conv2D(2**(nodeD+2), kernel_size=kernel, strides=stride, padding='same'))\n",
        "D.add(BatchNormalization(momentum=moment))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Conv2D(2**(nodeD+3), kernel_size=kernel, strides=stride-1, padding='same'))\n",
        "D.add(BatchNormalization(momentum=moment))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Flatten())\n",
        "D.add(Dense(1, activation='sigmoid'))\n",
        "D.compile(optimizer=keras.optimizers.Adam(lrD), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "z = keras.layers.Input(shape=(latent,))\n",
        "gen = G(z)\n",
        "D.trainable = False\n",
        "validity = D(gen)\n",
        "AM = keras.models.Model(z, validity)\n",
        "AM.compile(optimizer=keras.optimizers.Adam(lrG), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "''' Train '''\n",
        "Epc, DTy, DFy, GNy = [], [], [], []\n",
        "y_true = np.ones([batchs, 1])\n",
        "y_false = np.zeros([batchs, 1])\n",
        "k = 3\n",
        "for epoch in range(1, epochs+1):\n",
        "    X_real = dataset[np.random.randint(0,\n",
        "    dataset.shape[0],\n",
        "    size=batchs)]\n",
        "    X_noise = np.random.normal(0.0, 1.0, size=[batchs, latent])\n",
        "    X_fake = G.predict(X_noise)\n",
        "    dT_loss = D.train_on_batch(X_real, y_true)\n",
        "    dF_loss = D.train_on_batch(X_fake, y_false)\n",
        "    DT_loss = round(float(dT_loss[0]), 3)\n",
        "    DF_loss = round(float(dF_loss[0]), 3)\n",
        "    try: g_loss = [GNy[-1]]\n",
        "    except: g_loss = [0]\n",
        "    if epoch % (k+1) == 0:\n",
        "        g_loss = AM.train_on_batch(X_noise, y_true)\n",
        "    GN_loss = round(float(g_loss[0]), 3)\n",
        "    Epc.append(epoch)\n",
        "    DTy.append(DT_loss)\n",
        "    DFy.append(DF_loss)\n",
        "    GNy.append(GN_loss)\n",
        "    Verb = 'Epoch: {:6d} [DT {:.7f}][DF {:.7f}][G {:.7f}]'.format(epoch, DT_loss, DF_loss, GN_loss)\n",
        "    #print(Verb)\n",
        "G.save_weights('weights.h5')\n",
        "''' Generate '''\n",
        "noise = np.random.normal(0.0, 1.0, size=[1, latent])\n",
        "gen = G.predict(noise)\n",
        "P = gen[:,:,0]\n",
        "S = gen[:,:,1]\n",
        "C = gen[:,:,2:]\n",
        "P += 1\n",
        "S += 1\n",
        "C += 1\n",
        "P *= 180\n",
        "S *= 180\n",
        "C *= (C_MAX/2)\n",
        "P = np.reshape(P, (150,))\n",
        "S = np.reshape(S, (150,))\n",
        "C = np.reshape(C, (150, 150))\n",
        "fold(P, S, C)\n",
        "SQM('backbone.pdb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWiksT9EQqat"
      },
      "source": [
        "# **PS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf8XZYxOQqlE"
      },
      "source": [
        "def fold(P, S):\n",
        "    ''' Folds a structure using phi/psi angles '''\n",
        "    P = np.ndarray.tolist(P)\n",
        "    S = np.ndarray.tolist(S)\n",
        "    size = int(len(P))\n",
        "    Vs = []\n",
        "    for numb in range(size): Vs.append('A')\n",
        "    sequence = ''.join(Vs)\n",
        "    pose = pose_from_sequence(sequence)\n",
        "    for count, (phi, psi) in enumerate(zip(P, S)):\n",
        "        pose.set_phi(count+1, float(phi))\n",
        "        pose.set_psi(count+1, float(psi))\n",
        "    pose.dump_pdb('angles.pdb')\n",
        "    structure = Bio.PDB.PDBParser().get_structure('angles', 'angles.pdb')\n",
        "    dssp = Bio.PDB.DSSP(structure[0], 'angles.pdb', acc_array='Wilke')\n",
        "    ppb = Bio.PDB.Polypeptide.PPBuilder()\n",
        "    chain = ppb.build_peptides(structure, aa_only=False)[0]\n",
        "    SS = []\n",
        "    for aa in dssp:\n",
        "        if aa[2] == 'G' or aa[2] == 'H' or aa[2] == 'I': SSname = 'H'\n",
        "        elif aa[2] == 'B' or aa[2] == 'E': SSname = 'S'\n",
        "        else: SSname = 'L'\n",
        "        SS.append(SSname)\n",
        "    try:\n",
        "        for i in enumerate(reversed(SS)):\n",
        "            if i[1] != 'L':\n",
        "                num = i[0]\n",
        "                break\n",
        "        for model in structure:\n",
        "            for chain in model:\n",
        "                for i in reversed(range(150-num+1, 150+1)):\n",
        "                    chain.detach_child((' ', i, ' '))\n",
        "        io = Bio.PDB.PDBIO()\n",
        "        io.set_structure(structure)\n",
        "        io.save('turnicated.pdb')\n",
        "        os.remove('angles.pdb')\n",
        "        pose = pose_from_pdb('turnicated.pdb')\n",
        "    except:\n",
        "        os.remove('angles.pdb')\n",
        "        print('[-] Generated structure not satisfactory')\n",
        "        exit()\n",
        "    size = pose.residues.__len__()\n",
        "    with open('constraints.cst', 'w') as thefile:\n",
        "        for a in range(1, size+1):\n",
        "            for A in range(1, size+1):\n",
        "                if C[a][A] !=0:\n",
        "                    line = 'AtomPair CA {} CA {} GAUSSIANFUNC {} 1.0\\n'\\\n",
        "                    .format(a, A, C[a][A])\n",
        "                    thefile.write(line)\n",
        "    scorefxn = get_fa_scorefxn()\n",
        "    relax = pyrosetta.rosetta.protocols.relax.FastRelax()\n",
        "    relax.set_scorefxn(scorefxn)\n",
        "    os.remove('turnicated.pdb')\n",
        "    relax.apply(pose)\n",
        "    pose.dump_pdb('backbone.pdb')\n",
        "\n",
        "lrG = 0.001\n",
        "lrD = 0.001\n",
        "nodeG = 6\n",
        "nodeD = 5\n",
        "moment = 0.8\n",
        "alpha = 0.2\n",
        "drop = 0.25\n",
        "kernel = 3\n",
        "stride = 2\n",
        "latent = 128\n",
        "batchs = 64\n",
        "epochs = 10\n",
        "with h5py.File('PS+CM.hdf5', 'r') as data: dataset=data['default'][()]\n",
        "dataset = dataset[:, :, :2]\n",
        "dataset = np.reshape(dataset, (-1, dataset.shape[1], dataset.shape[2], 1))\n",
        "shape = dataset.shape[1:]\n",
        "print(dataset.shape)\n",
        "G = Sequential()\n",
        "G.add(Dense(2**(nodeG+1) * 75 * 1, activation='relu',input_dim=latent))\n",
        "G.add(Reshape((75, 1, 2**(nodeG+1))))\n",
        "G.add(UpSampling2D(size=(1, 1)))\n",
        "G.add(Conv2D(2**(nodeG+1), kernel_size=kernel, padding='same'))\n",
        "G.add(BatchNormalization(momentum=moment))\n",
        "G.add(Activation('relu'))\n",
        "G.add(UpSampling2D())\n",
        "G.add(Conv2D(2**(nodeG+0), kernel_size=kernel, padding='same'))\n",
        "G.add(BatchNormalization(momentum=moment))\n",
        "G.add(Activation('relu'))\n",
        "G.add(Conv2D(1, kernel_size=kernel, padding='same'))\n",
        "G.add(Activation('tanh'))\n",
        "D = Sequential()\n",
        "D.add(Conv2D(2**(nodeD+0), kernel_size=kernel, strides=stride, input_shape=shape, padding='same'))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Conv2D(2**(nodeD+1), kernel_size=kernel, strides=stride, padding='same'))\n",
        "D.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "D.add(BatchNormalization(momentum=moment))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Conv2D(2**(nodeD+2), kernel_size=kernel, strides=stride, padding='same'))\n",
        "D.add(BatchNormalization(momentum=moment))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Conv2D(2**(nodeD+3), kernel_size=kernel, strides=stride-1, padding='same'))\n",
        "D.add(BatchNormalization(momentum=moment))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Flatten())\n",
        "D.add(Dense(1, activation='sigmoid'))\n",
        "D.compile(optimizer=keras.optimizers.Adam(lrD), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "z = keras.layers.Input(shape=(latent,))\n",
        "gen = G(z)\n",
        "D.trainable = False\n",
        "validity = D(gen)\n",
        "AM = keras.models.Model(z, validity)\n",
        "AM.compile(optimizer=keras.optimizers.Adam(lrG), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "''' Train '''\n",
        "Epc, DTy, DFy, GNy = [], [], [], []\n",
        "y_true = np.ones([batchs, 1])\n",
        "y_false = np.zeros([batchs, 1])\n",
        "k = 3\n",
        "for epoch in range(1, epochs+1):\n",
        "    X_real = dataset[np.random.randint(0,\n",
        "    dataset.shape[0],\n",
        "    size=batchs)]\n",
        "    X_noise = np.random.normal(0.0, 1.0, size=[batchs, latent])\n",
        "    X_fake = G.predict(X_noise)\n",
        "    dT_loss = D.train_on_batch(X_real, y_true)\n",
        "    dF_loss = D.train_on_batch(X_fake, y_false)\n",
        "    DT_loss = round(float(dT_loss[0]), 3)\n",
        "    DF_loss = round(float(dF_loss[0]), 3)\n",
        "    try: g_loss = [GNy[-1]]\n",
        "    except: g_loss = [0]\n",
        "    if epoch % (k+1) == 0:\n",
        "        g_loss = AM.train_on_batch(X_noise, y_true)\n",
        "    GN_loss = round(float(g_loss[0]), 3)\n",
        "    Epc.append(epoch)\n",
        "    DTy.append(DT_loss)\n",
        "    DFy.append(DF_loss)\n",
        "    GNy.append(GN_loss)\n",
        "    Verb = 'Epoch: {:6d} [DT {:.7f}][DF {:.7f}][G {:.7f}]'.format(epoch, DT_loss, DF_loss, GN_loss)\n",
        "    print(Verb)\n",
        "''' Generate '''\n",
        "noise = np.random.normal(0.0, 1.0, size=[1, latent])\n",
        "gen = G.predict(noise)\n",
        "P = gen[:,:,0]\n",
        "S = gen[:,:,1]\n",
        "P += 1\n",
        "S += 1\n",
        "P *= 180\n",
        "S *= 180\n",
        "P = np.reshape(P, (150,))\n",
        "S = np.reshape(S, (150,))\n",
        "fold(P, S)\n",
        "SQM('backbone.pdb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vaect4G7Qq5b"
      },
      "source": [
        "# **CM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RHyPiOkRzFj"
      },
      "source": [
        "def fold(C):\n",
        "    ''' Folds a structure using contact map '''\n",
        "    size = 150\n",
        "    Vs = []\n",
        "    for numb in range(size): Vs.append('A')\n",
        "    sequence = ''.join(Vs)\n",
        "    pose = pose_from_sequence(sequence)\n",
        "    con = pyrosetta.rosetta.protocols.constraint_movers.ConstraintSetMover()\n",
        "    con.constraint_file('constraints.cst')\n",
        "    con.add_constraints(True)\n",
        "    con.apply(pose)\n",
        "    scorefxn = get_fa_scorefxn()\n",
        "    score_manager = pyrosetta.rosetta.core.scoring.ScoreTypeManager()\n",
        "    atom_pair_constraint = score_manager.score_type_from_name('atom_pair_constraint')\n",
        "    rama_prepro = score_manager.score_type_from_name('rama_prepro')\n",
        "    scorefxn.set_weight(atom_pair_constraint, 5)\n",
        "    scorefxn.set_weight(rama_prepro, 5)\n",
        "    relax = pyrosetta.rosetta.protocols.relax.FastRelax()\n",
        "    relax.set_scorefxn(scorefxn)\n",
        "    os.remove('constraints.cst')\n",
        "    relax.apply(pose)\n",
        "    pose.dump_pdb('backbone.pdb')\n",
        "\n",
        "lrG = 0.001\n",
        "lrD = 0.001\n",
        "nodeG = 6\n",
        "nodeD = 5\n",
        "moment = 0.8\n",
        "alpha = 0.2\n",
        "drop = 0.25\n",
        "kernel = 3\n",
        "stride = 2\n",
        "latent = 128\n",
        "batchs = 64\n",
        "epochs = 10\n",
        "C_MAX = 12\n",
        "with h5py.File('PS+CM.hdf5', 'r') as data: dataset=data['default'][()]\n",
        "dataset = dataset[:, :, 2:152]\n",
        "dataset = np.reshape(dataset, (-1, dataset.shape[1], dataset.shape[2], 1))\n",
        "shape = dataset.shape[1:]\n",
        "print(dataset.shape)\n",
        "G = Sequential()\n",
        "G.add(Dense(2**(nodeG+1) * 75 * 75, activation='relu',input_dim=latent))\n",
        "G.add(Reshape((75, 75, 2**(nodeG+1))))\n",
        "G.add(UpSampling2D(size=(1, 1)))\n",
        "G.add(Conv2D(2**(nodeG+1), kernel_size=kernel, padding='same'))\n",
        "G.add(BatchNormalization(momentum=moment))\n",
        "G.add(Activation('relu'))\n",
        "G.add(UpSampling2D())\n",
        "G.add(Conv2D(2**(nodeG+0), kernel_size=kernel, padding='same'))\n",
        "G.add(BatchNormalization(momentum=moment))\n",
        "G.add(Activation('relu'))\n",
        "G.add(Conv2D(1, kernel_size=kernel, padding='same'))\n",
        "G.add(Activation('tanh'))\n",
        "D = Sequential()\n",
        "D.add(Conv2D(2**(nodeD+0), kernel_size=kernel, strides=stride, input_shape=shape, padding='same'))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Conv2D(2**(nodeD+1), kernel_size=kernel, strides=stride, padding='same'))\n",
        "D.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "D.add(BatchNormalization(momentum=moment))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Conv2D(2**(nodeD+2), kernel_size=kernel, strides=stride, padding='same'))\n",
        "D.add(BatchNormalization(momentum=moment))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Conv2D(2**(nodeD+3), kernel_size=kernel, strides=stride-1, padding='same'))\n",
        "D.add(BatchNormalization(momentum=moment))\n",
        "D.add(LeakyReLU(alpha=alpha))\n",
        "D.add(Dropout(drop))\n",
        "D.add(Flatten())\n",
        "D.add(Dense(1, activation='sigmoid'))\n",
        "D.compile(optimizer=keras.optimizers.Adam(lrD), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "z = keras.layers.Input(shape=(latent,))\n",
        "gen = G(z)\n",
        "D.trainable = False\n",
        "validity = D(gen)\n",
        "AM = keras.models.Model(z, validity)\n",
        "AM.compile(optimizer=keras.optimizers.Adam(lrG), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "''' Train '''\n",
        "Epc, DTy, DFy, GNy = [], [], [], []\n",
        "y_true = np.ones([batchs, 1])\n",
        "y_false = np.zeros([batchs, 1])\n",
        "k = 3\n",
        "for epoch in range(1, epochs+1):\n",
        "    X_real = dataset[np.random.randint(0,\n",
        "    dataset.shape[0],\n",
        "    size=batchs)]\n",
        "    X_noise = np.random.normal(0.0, 1.0, size=[batchs, latent])\n",
        "    X_fake = G.predict(X_noise)\n",
        "    dT_loss = D.train_on_batch(X_real, y_true)\n",
        "    dF_loss = D.train_on_batch(X_fake, y_false)\n",
        "    DT_loss = round(float(dT_loss[0]), 3)\n",
        "    DF_loss = round(float(dF_loss[0]), 3)\n",
        "    try: g_loss = [GNy[-1]]\n",
        "    except: g_loss = [0]\n",
        "    if epoch % (k+1) == 0:\n",
        "        g_loss = AM.train_on_batch(X_noise, y_true)\n",
        "    GN_loss = round(float(g_loss[0]), 3)\n",
        "    Epc.append(epoch)\n",
        "    DTy.append(DT_loss)\n",
        "    DFy.append(DF_loss)\n",
        "    GNy.append(GN_loss)\n",
        "    Verb = 'Epoch: {:6d} [DT {:.7f}][DF {:.7f}][G {:.7f}]'.format(epoch, DT_loss, DF_loss, GN_loss)\n",
        "    print(Verb)\n",
        "''' Generate '''\n",
        "noise = np.random.normal(0.0, 1.0, size=[1, latent])\n",
        "gen = G.predict(noise)\n",
        "C = gen\n",
        "C += 1\n",
        "C *= (C_MAX/2)\n",
        "C = np.reshape(C, (150, 150))\n",
        "fold(C)\n",
        "SQM('backbone.pdb')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}